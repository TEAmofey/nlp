{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qclzljFDqKk3"
      },
      "source": [
        "# Speculative Decoding\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2024/week10_efficiency/hw_speculative.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6u2Z05JsvlL",
        "outputId": "2632122a-816e-4174-a054-df03eef032b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.6/209.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:31\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install triton\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahhcFqfBqKk4"
      },
      "source": [
        "### Greedy Generation Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAoayJ-GqKk5"
      },
      "source": [
        "**Load the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX4Fg1aUqKk5"
      },
      "outputs": [],
      "source": [
        "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
        "%env CUDA_VISIBLE_DEVICES=0 # Change it if you're on a multy-GPU machine\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "LLAMA_REPO = \"unsloth/Llama-3.2-3B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA_REPO, torch_dtype=torch.float16, device_map=\"cuda\")\n",
        "model.generation_config.pad_token_id = 128001\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLAMA_REPO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tABKQ69wqKk6"
      },
      "source": [
        "**Compile model forward for more accurate benchmarks**\n",
        "\n",
        "`transformers` is not a very efficient inference engine because of high python overhead and almost no kernel optimization.\n",
        "\n",
        "However, with `PyTorch` `v2.0.0`, [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) was introduced. This feature allows for capturing, isolating and optimizing CUDA runtime in `PyTorch`. Using this feature, we can effectively eliminate almost all python overhead and optimize the kernels.\n",
        "\n",
        "Starting with [`transformers` `v4.44.0`](https://github.com/huggingface/transformers/releases/tag/v4.44.0), this feature is integrated with `transformers` text generation utils end-to-end. However, for simplicity, we'll apply it to the forward pass of the model specifically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QLsL7m0qKk6"
      },
      "outputs": [],
      "source": [
        "model.forward = torch.compile(\n",
        "    model.forward,          # the function call to compile\n",
        "    fullgraph=True,         # Compile all the CUDA kernels into a single entity\n",
        "    mode=\"reduce-overhead\", # Optimize for speed\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x5ojJbrqKk7"
      },
      "source": [
        "**Benchmark forward passes with different seq_len**\n",
        "\n",
        "Run the following cell 2+ times. The first time is slow because that's when the compilation is taking place.\n",
        "\n",
        "The following runs are much faster.\n",
        "\n",
        "(EXTRA: run this cell without compiling to measure scompilation speedup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y_geHUkqKk7",
        "outputId": "120bc5df-4cef-4aee-8dee-227e8720c8a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 tokens: 31.35 passes/s\n",
            "2 tokens: 33.34 passes/s\n",
            "4 tokens: 32.99 passes/s\n",
            "8 tokens: 32.37 passes/s\n",
            "16 tokens: 31.21 passes/s\n",
            "32 tokens: 25.95 passes/s\n",
            "64 tokens: 23.99 passes/s\n",
            "128 tokens: 18.13 passes/s\n",
            "256 tokens: 9.83 passes/s\n",
            "512 tokens: 5.19 passes/s\n",
            "1024 tokens: 2.47 passes/s\n"
          ]
        }
      ],
      "source": [
        "from time import perf_counter\n",
        "\n",
        "NUM_REPEATS = 100\n",
        "\n",
        "throughpus = {}\n",
        "\n",
        "for seq_len in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
        "    input_ids = torch.randint(0, tokenizer.vocab_size, (1, seq_len)).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start = perf_counter()\n",
        "        for _ in range(NUM_REPEATS):\n",
        "            model(\n",
        "                input_ids,\n",
        "                use_cache=False,\n",
        "            )\n",
        "            torch.cuda.synchronize()\n",
        "        end = perf_counter()\n",
        "    throughpus[seq_len] = NUM_REPEATS * seq_len / (end - start)\n",
        "    print(f\"{seq_len} tokens: {NUM_REPEATS / (end - start):.2f} passes/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-qz7ahOqKk8"
      },
      "source": [
        "As we can see, the forward pass speed almost doesn't depend on the number of tokens passed through the model up to around **16** tokens at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwAeHLbAqKk9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(throughpus.keys(), throughpus.values())\n",
        "plt.xscale(\"log\")\n",
        "plt.yscale(\"log\")\n",
        "\n",
        "plt.xlabel(\"Tokens in pass\")\n",
        "plt.ylabel(\"Troughput, tokens per second\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_TbvSatqKk9"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "model = AutoModelForCausalLM.from_pretrained(LLAMA_REPO, torch_dtype=torch.float16, device_map=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFIT6AmUqKk9"
      },
      "source": [
        "## Speculative Decoding\n",
        "\n",
        "As a baseline, we'll generate hypotheses using a very simple bigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7JF2jQqKk-"
      },
      "source": [
        "### Load the Data\n",
        "\n",
        "We'll use the [wikitext2](https://paperswithcode.com/dataset/wikitext-2) dataset as a sample of natural language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk8KcWC4qKk-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "SEED = 0\n",
        "\n",
        "def get_wikitext2(seed, seqlen, nsamples=64):\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLAMA_REPO, use_fast=False)\n",
        "\n",
        "    train_input_ids = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt').input_ids\n",
        "    random.seed(seed)\n",
        "    train_batch = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, train_input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = train_input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        train_batch.append(inp[0])\n",
        "\n",
        "    test_input_ids = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt').input_ids\n",
        "    test_input_ids = test_input_ids[:, :(test_input_ids.shape[1] // seqlen) *  seqlen]\n",
        "    test_input_ids = test_input_ids.reshape(test_input_ids.shape[1] // seqlen, seqlen)\n",
        "\n",
        "    return torch.stack(train_batch), test_input_ids\n",
        "\n",
        "train_batch, test_input_ids = get_wikitext2(SEED, 8192)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2K-tFOxyqKk-"
      },
      "source": [
        "**Task (0.5pt points):** Build a bigram model\n",
        "\n",
        "Using sequences from `train_batch`, build a bigram model for predicting `n` tokens into the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anchr3_TqKk-"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from typing import Mapping\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_next_token_array(train_data, vocab_size: int=128256, default_next_token: int=220) -> Mapping[int, int]:\n",
        "    \"\"\"\n",
        "    Builds an array mapping each token in the vocabulary to its most likely next token based on training data.\n",
        "\n",
        "    Args:\n",
        "        train_data (np.ndarray): Array of training data tokens.\n",
        "        vocab_size (int): The size of the vocabulary.\n",
        "        default_next_token (int): Default token to use if no next token can be determined.\n",
        "\n",
        "    Returns:\n",
        "        Mapping[int, int]: Array where each index represents a token and the value at that index is the most likely next token.\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    # Step 1: Prepare the Data\n",
        "\n",
        "    # Step 2: Create Pairs and Count Occurrences\n",
        "\n",
        "    # Step 3: Build Mapping from Current Token to Next Token Counts\n",
        "\n",
        "    # Step 4: Determine the Most Likely Next Token\n",
        "\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "    return next_tokens_array\n",
        "\n",
        "def speculate_bigram(input_ids: torch.Tensor, position: int, n: int, next_tokens_array: Mapping[int, int]) -> int:\n",
        "    \"\"\"\n",
        "    Generates a speculative sequence by predicting next tokens in a sequence using a bigram model.\n",
        "\n",
        "    Args:\n",
        "        input_ids (torch.Tensor): Tensor of input token IDs.\n",
        "        position (int): Position in the sequence to begin speculation.\n",
        "        n (int): Number of tokens to generate.\n",
        "        next_tokens_array (Mapping[int, int]): Mapping of tokens to their most likely next token.\n",
        "\n",
        "    Returns:\n",
        "        int: Number of tokens generated.\n",
        "    \"\"\"\n",
        "    for i in range(n):\n",
        "        hypo_next_token = next_tokens_array[input_ids[0, position - 1].item()]\n",
        "        input_ids[0, position] = hypo_next_token\n",
        "        position += 1\n",
        "    return n\n",
        "\n",
        "\n",
        "NEXT_TOKEN_WIKI2 = build_next_token_array(train_batch.flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZP5SyfXqKk_"
      },
      "source": [
        "**Task (1.5 points)** Implement greedy sequential speculative decoding:\n",
        "\n",
        "You're given a prototype of the function that generate a token sequence greedily by speculating `n` tokens into the fulure and verifying those tokens.\n",
        "\n",
        "Your task is to:\n",
        " * Correctly fill a hypothesis inplace (using `speculate_fn`)\n",
        " * Pass the hypothesis through the model (with correct `past_key_values`)\n",
        " * Find where the hypothesis diverges.\n",
        " * Update the current position in the generation, as well as the number of forward pass calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjF97B3gqKk_",
        "outputId": "1988ae5a-29eb-40a8-9ae1-b54ca577d0f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The 'max_batch_size' argument of StaticCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HIT: ' is'->' a'\n",
            "HIT: ' region'->' of the'\n",
            "HIT: ' United'->' States'\n",
            "HIT: ' home'->' to'\n",
            "HIT: ' number'->' of'\n",
            "HIT: ' some'->' of the'\n",
            "HIT: ' in'->' the'\n",
            "HIT: ' is'->' a'\n",
            "HIT: ' destination'->' for'\n",
            "HIT: ' interested'->' in'\n",
            "HIT: ' home'->' to the'\n",
            "Tokens per forward pass: 1.1627906976744187\n",
            "\n",
            "<|begin_of_text|>The Pacific Northwest is a region of the United States that is known for its natural beauty, diverse landscapes, and rich cultural heritage. The region is home to a number of unique and interesting places that are worth visiting. In this article, we will explore some of the best places to visit in the Pacific Northwest.\n",
            "1. Mount Rainier National Park: Located in Washington state, Mount Rainier National Park is a must-visit destination for anyone interested in natural beauty. The park is home to the tallest mountain in\n"
          ]
        }
      ],
      "source": [
        "from transformers import StaticCache\n",
        "\n",
        "\n",
        "def truncate_past_key_values(past_key_values, position):\n",
        "    \"\"\"\n",
        "    Truncates the past key and value caches at a specific position. `transformers`-specific stuff. Might change with their next update.\n",
        "\n",
        "    Args:\n",
        "        past_key_values (object): Object containing key and value caches for each layer.\n",
        "        position (int): Position from which to truncate the caches.\n",
        "    \"\"\"\n",
        "    for layer_idx in range(len(past_key_values.key_cache)):\n",
        "        past_key_values.key_cache[layer_idx][:,:,position - 1:] = 0.0\n",
        "        past_key_values.value_cache[layer_idx][:,:,position - 1:] = 0.0\n",
        "\n",
        "\n",
        "def generate_speculative(model, tokenizer, num_tokens_to_generate: int, speculate_fn: callable, max_speculated_len: int=128, initial_prompt: str=\"The Pacific\", verbose: bool=False):\n",
        "    \"\"\"\n",
        "    Generates text using speculative decoding, a technique that combines conventional forward passes with speculative predictions to reduce computation by hypothesizing multiple tokens at each step.\n",
        "\n",
        "    Args:\n",
        "        model (transformers.PreTrainedModel): The language model used to generate text.\n",
        "        tokenizer (transformers.PreTrainedTokenizer): Tokenizer corresponding to the model for encoding and decoding text.\n",
        "        num_tokens_to_generate (int): The number of tokens to generate in total.\n",
        "        speculate_fn (callable): A function that generates speculative tokens based on the input IDs and current position.\n",
        "            This function takes `input_ids` and `position` as arguments and returns the number of tokens speculated.\n",
        "        max_speculated_len (int, optional): The maximum length of speculative tokens allowed in a single step. Defaults to 128.\n",
        "        initial_prompt (str, optional): The starting prompt for text generation. Defaults to \"The Pacific\".\n",
        "        verbose (bool, optional): If True, prints debugging information about successful speculative predictions. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The generated sequence of input IDs up to the generated position.\n",
        "        int: The number of forward passes required for generation, which can be used to assess efficiency.\n",
        "\n",
        "    Example:\n",
        "        >>> output, forward_passes = generate_speculative(\n",
        "        ...     model,\n",
        "        ...     tokenizer,\n",
        "        ...     num_tokens_to_generate=100,\n",
        "        ...     speculate_fn=lambda input_ids, position: fill_hypothesis(input_ids, position, n=2, next_tokens_array=next_tokens_array),\n",
        "        ...     verbose=True\n",
        "        ... )\n",
        "        >>> print(\"Generated Text:\", tokenizer.decode(output))\n",
        "        >>> print(\"Tokens per forward pass:\", 100 / forward_passes)\n",
        "\n",
        "    Notes:\n",
        "        - Speculative decoding reduces the number of model passes by hypothesizing tokens using the `speculate_fn` function. The actual model output is used to verify these hypotheses, allowing efficient token generation.\n",
        "        - If verbose mode is enabled, it will display speculative matches as they occur.\n",
        "    \"\"\"\n",
        "\n",
        "    # Encode the initial prompt and get input IDs, then move them to the GPU\n",
        "    prompt_ids = tokenizer(initial_prompt, return_tensors=\"pt\", truncation=True).input_ids.to(\"cuda\")\n",
        "    position = prompt_ids.shape[1]  # Initial position for the prompt\n",
        "\n",
        "    # Calculate the maximum cache length to accommodate generated tokens and speculative tokens\n",
        "    max_cache_len = num_tokens_to_generate + position + max_speculated_len\n",
        "    # Initialize the cache for past key values with model configuration, setting cache size and device\n",
        "    past_key_values = StaticCache(config=model.config, max_batch_size=1, max_cache_len=max_cache_len, device=\"cuda\", dtype=torch.float16)\n",
        "    # Allocate space for input IDs and cache positions on the GPU\n",
        "    input_ids = torch.zeros((1, max_cache_len), device=\"cuda\", dtype=torch.long)\n",
        "    # Fill in the initial prompt\n",
        "    input_ids[0, :position] = prompt_ids[0, :position]\n",
        "\n",
        "    with torch.no_grad():  # Disable gradients for inference\n",
        "        # Pre-fill cache with the prompt to start the model's internal state\n",
        "        past_key_values = model(input_ids[:, :position], past_key_values=past_key_values).past_key_values\n",
        "\n",
        "        forward_passes = 0  # Track number of forward passes\n",
        "        tokens_generated = 0  # Track number of tokens generated\n",
        "        while tokens_generated < num_tokens_to_generate:\n",
        "            # Adjust cache by removing outdated values to avoid memory overflow\n",
        "            truncate_past_key_values(past_key_values, position)\n",
        "\n",
        "            # Speculate the next few tokens based on current position\n",
        "            num_tokens_speculated = speculate_fn(input_ids, position)  # Number of tokens hypothesized\n",
        "            # YOUR CODE HERE>>>>>>>>>\n",
        "            output = model(\n",
        "                ...\n",
        "            )\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "            past_key_values = output.past_key_values  # Update the cache with new predictions\n",
        "\n",
        "            # Extract predicted tokens and the speculative tokens for comparison\n",
        "            # YOUR CODE HERE>>>>>>>>>\n",
        "            real_tokens =\n",
        "            hypothesis_tokens =\n",
        "\n",
        "            match_len =\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "            # Optionally, print successful speculative predictions\n",
        "            if verbose and match_len > 0:\n",
        "                print(\n",
        "                    f\"HIT: '{tokenizer.decode(input_ids...)}'->'{tokenizer.decode(real_tokens...)}'\"\n",
        "                )\n",
        "\n",
        "            # Copy matched tokens to the input IDs array\n",
        "            # YOUR CODE HERE>>>>>>>>>\n",
        "            input_ids... =\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "\n",
        "            # Update the position, tokens generated count, and forward passes\n",
        "            # YOUR CODE HERE>>>>>>>>>\n",
        "            position +=\n",
        "            forward_passes +=\n",
        "            tokens_generated +=\n",
        "            # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "    return input_ids[0, :position], forward_passes\n",
        "\n",
        "NUM_TOKENS_TO_GENERATE = 100\n",
        "\n",
        "# Call the function with parameters and lambda function for speculative decoding\n",
        "output, forward_passes = generate_speculative(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    NUM_TOKENS_TO_GENERATE,\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    speculate_fn=,\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "    verbose=True)\n",
        "\n",
        "# Display metrics and the decoded output\n",
        "print(f\"Tokens per forward pass: {NUM_TOKENS_TO_GENERATE / forward_passes}\\n\")\n",
        "print(tokenizer.decode(output))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSbqLx7lqKlA"
      },
      "outputs": [],
      "source": [
        "output0, n0 = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, 0, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "output1, n1 = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, 1, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "output2, n2 = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, 2, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "output4, n4 = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, 4, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "\n",
        "try:\n",
        "    assert n0 == NUM_TOKENS_TO_GENERATE, \"Model that doesnt' speculate does exatcly one forward pass per token\"\n",
        "    assert n4 <= n2 and n2 <= n1 and n1 < n0, \"It's very unlikely that the performance decreases with stronger speculation\"\n",
        "    assert (output1 == output0).all(), \"The outputs don't match\"\n",
        "    print(\"All tests passed\")\n",
        "except AssertionError as e:\n",
        "    print(\"Error occured. Generated texts:\\n\")\n",
        "    print(*tokenizer.batch_decode([output0, output1, output2, output4]), sep=\"\\n\", end=\"\\n\\n\")\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0TPKWJaqKlA"
      },
      "source": [
        "As we have shown, we can increase the hypotheses length up to `16` at almost no inference cost.\n",
        "\n",
        "Let's see how much it actually helps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIplTHc7qKlA",
        "outputId": "fba7223f-f8c0-4af4-dbdc-d629e211363e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speculate length 0: 1.00 tokens per forward pass\n",
            "Speculate length 1: 1.14 tokens per forward pass\n",
            "Speculate length 2: 1.16 tokens per forward pass\n",
            "Speculate length 3: 1.16 tokens per forward pass\n",
            "Speculate length 4: 1.16 tokens per forward pass\n",
            "Speculate length 5: 1.16 tokens per forward pass\n",
            "Speculate length 6: 1.16 tokens per forward pass\n"
          ]
        }
      ],
      "source": [
        "for speculate_length in [0, 1, 2, 3, 4, 5, 6]:\n",
        "    _, num_passes = generate_speculative(model, tokenizer, NUM_TOKENS_TO_GENERATE, speculate_fn=lambda input_ids, position: speculate_bigram(input_ids, position, speculate_length, NEXT_TOKEN_WIKI2), verbose=False)\n",
        "    print(f\"Speculate length {speculate_length}: {NUM_TOKENS_TO_GENERATE / num_passes:.2f} tokens per forward pass\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T48EpfDmqKlA"
      },
      "source": [
        "It appears the bigram model doesn't improve above `2`. That is because it's very unlikely that a bigram model outputs a long meaningful sequence.\n",
        "\n",
        "We need a stronger model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eupNMG2qKlA"
      },
      "source": [
        "<span style=\"color:Red\">SEMINAR ENDS HERE HOPEFULLY</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS4KLXTwqKlA"
      },
      "source": [
        "## Stronger Model\n",
        "\n",
        "**Task (2.0+ points)** Use a stronger model for speculative decoding:\n",
        "\n",
        "The `generate_speculative` function can take any callable inplace of `speculate_fn`. You have many options as what can be used here:\n",
        " * **Larger N-gram models**: use larger n and more data to fit it.\n",
        " * **A tiny Llama model**: the problem with using an LLM as a speculative generator is that it has to share tokenizer with the verifier model. That is, your tiny llama has to have the same tokenizer.\n",
        "   * Make sure to properly store and reuse `past_key_values` for it.\n",
        "   * Verify that the model forward pass is really negligible in terms of hypothesis generation time (time verify >> time speculate)\n",
        " * **An LSTM**: Train it from scratch. You might initialize the embedding layer with the llama embeddings for easier convergence.\n",
        " * **word2vec/CBOW**: Train it on the `train_batch` or anything else you want. Make sure that it only uses past tokens and that the context is longer than what you want to speculate.\n",
        " * **Wide Speculation**: Your hypothesis doesn't have to be linear. You can actually speculate a tree of possible next tokens and verify it with a single pass. That requires some nontrivial `attention_mask` manipulations, however. Read more [here](https://arxiv.org/abs/2305.09781) and [here](https://huggingface.co/blog/poedator/4d-masks).\n",
        "\n",
        "*The amount of points to be received here depends on how complex your solution is and what token acceptance rate per forward pass of the verifier it achieves. Achieving accepting 2+ tokens per forward pass on average guarantees at leas 2 points.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HHjWNT9qKlB"
      },
      "outputs": [],
      "source": [
        "# A LOT OF YOUR CODE HERE\n",
        "\n",
        "super_duper_speculator ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5MZONgCqKlB"
      },
      "outputs": [],
      "source": [
        "NUM_TOKENS_TO_GENERATE = 100\n",
        "\n",
        "# Call the function with parameters and lambda function for speculative decoding\n",
        "output, forward_passes = generate_speculative(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    NUM_TOKENS_TO_GENERATE,\n",
        "    # YOUR CODE HERE>>>>>>>>>\n",
        "    speculate_fn=super_duper_speculator,\n",
        "    # <<<<<<<<<<<<<<<<<<<<<<<\n",
        "    verbose=True)\n",
        "\n",
        "# Display metrics and the decoded output\n",
        "print(f\"Tokens per forward pass: {NUM_TOKENS_TO_GENERATE / forward_passes}\\n\")\n",
        "print(tokenizer.decode(output))\n",
        "\n",
        "if NUM_TOKENS_TO_GENERATE / forward_passes >= 2.0:\n",
        "    print(\"Great Success!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}